---
layout:     post   				    # 使用的布局（不需要改）
title:      XLNet总结	# 标题 
subtitle:   Attention Mask部分的个人理解				 #副标题
date:       2019-07-02 				# 时间
update_date:       2019-07-02 				# 时间
author:     pfan8 						# 作者
header-img: img/post-bg-nlp.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags: [NLP, SOTA]
---

XLNet是CMU提出的基于BERT思想的改进模型，在各大NLP测试集中表现优异，此处不详细介绍模型，想要了解可以看<https://zhuanlan.zhihu.com/p/70220951>以及<https://cloud.tencent.com/developer/article/1454452>，已经介绍得相当全面了，这里总结一下自己的理解。

## Attention Mask

感叹XLNet中利用工程手段实现类似于BERT中打乱上下文顺序的效果，确实很巧妙，如下图所示

![](https://ask.qcloudimg.com/http-save/yehe-2194270/ra1zldsa1.jpeg?imageView2/2/w/1620)

以Content stream为例，利用二维矩阵，对于t时刻的X，通过给$X_t$周围的输入X打mask实现乱序的效果。假设原始输入顺序是1-2-3-4，XLNet想要实现实现的排序是3-2-4-1，那么对于$X_1$（第一排），实际上被调整到最后时刻了，因此$X_1$可以看到其他所有时刻的信息，同理对于$X_2$（第二排），由于在第二时刻，只能看到$X_3$，因此只能看到红点标出的$X_2$和$X_3$。放大可以看下图，白色代表mask的部分，红色代表能看到的部分：

![](https://thumbsnap.com/i/Jw2t9pYI.png?0702)

至于为什么要这么做，那两篇文章已经说得很清楚了，由于BERT打乱了自回归语言模型（LM）的自然次序，而LM在生成式的NLP任务上表现较好，因此为了融合两者优点，发明了Attention-Mask机制，也就实现了XLNet的Permutation Language Model（PLM）这一新的预训练模式，同时解决了pretrain和fine-tuning在BERT下不统一的问题。

## 问题

不过有一个问题文章没有说清楚，那就是pretrain中使用了PLM打乱顺序，那在fine-tuning中是否也要使用PLM呢？还是说按照天然次序即可（Attention只处理t时刻之前的输入），原文可能有讲，那还需要后续有空再熟读paper，不过也有可能是作者留给后人思考的问题: )